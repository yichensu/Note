# 大數據＿資料分析

## Term Weighting and VSM
information retrieval(IR)是文字探勘之基礎：通常是為了從大量語料集合中（通常非結構），發現滿足資訊需求者，先找出相關文章後，才能萃取自己想要的部分，去做text mining
例：在1650年若要演莎士比亞，去挑出有不同角色的劇本
* 缺：若用掃描曠日廢時，且只能用關鍵字比對，比較語意層級，無法進一步分析
* 另一作法：**Term-document incidence matrix（詞語文件的共現矩陣）**，可以一軸放角色、一軸放劇本，用布林值標記，知道哪些字詞出現在哪些文件
   * 布林值是一個向量，可以抽出向量，若不要的就0 1交換，做補數運算（不要「A角色」，就把A為0的換成1，1換成0）
   * 若共同都出現1，就會是答案，不用全部掃描
   * 但如果數量太多就會很難用，常見英文字40-50萬字詞，但有值者大概1/500，出現「稀疏矩陣」（僅有非常少數有0），造成資源浪費，因此現在大多採用Inverted Index
   ### Inverted index：先對文件編號，只記錄出現1的地方，可以省掉很多0的空間
   ![常見inverted index概念](https://github.com/yichensu/Note/blob/master/photo/%E5%B8%B8%E8%A6%8Binverted%20index%E6%A6%82%E5%BF%B5.png)
   * 標題是dict，儲存其(sorted)postings
   ### ranked retrival
   * 傳統布林查詢僅適合專家查詢，並不適合大眾
      * 布林運算不是0就是1，很常見因為條件設定不是太多就是太少（不會設關鍵字），很難控制
      * 比較適合專家檢索（已經知道要用什麼關鍵字），並不適合門外和壹般性查詢
   * ranked retrival是一種折衷，但要解決相關性排序問題
      * 折衷方式是排序，較符合放前面，通常八成都只會看第一頁結果即使有幾十幾百萬資料
      * 最大問題是「什麼是比較相關」
      * 做法：可以有相關詞就給1分，欲相關分數愈多
        #### Term freqeuncy: 從布林值，改成劇本中出現特定詞幾次，為標記為0，有標記舊紀錄次數（直接用次數看，人看很快）
        1. Here is Bag of Word(BOW) model
            + 文章剪碎，去看出現重量（出現幾次），不管相對順序(知順序：sequential model)
            + 限制是不知道順序但比較簡單（continuous model)
        2. 相關程度：出現符合自較多放前面，符合較少放後面，除非有同義詞，不然tf為0就是不相關
        3. 但資訊增量，差1愈沒差，會逐漸飽和
        4. 最常用法式會取log（以多少為底都可），若有出現就會有1，後面會取log
        5. 例：想知道是否有取到特定字，如目標為「information on cars」，會分開`information` 、`on`、`cars`去對應標題
           * 對象一：all you have ever wanted to know about cars >> `tf = 0 + 0 + (1+log1)`
           * 對象二：information on trucks, information on planes, information on trains >> `tf = (1+log3) + (1+log3) + 0`
           * 但是這樣很怪，因為他們不應該等重，像cars比較重要，所以要算權重
        #### tf-idf weighting
        * 常見字會被認為是stopword，英文字大概有兩百多個，包含`I`、`you`、`of`、`at`、`on` 等等，高頻但沒鑑別力，所得到資訊量較少（通常人在看會自動過濾掉字詞）
        * 生字會被認為有特別意義，所以希望將生字權重加高（人看到不熟悉的字彙很好奇）
        * 這些自有辨識能力，可辨識某一意義
        1. Document Frequency（某字詞在幾篇文章中出現） 
            + df算法：只要出現在某篇文獻中就+1，但只算一次
            + 希望稀有字提高權重，希望常見字降低權重，所以要將df取倒數，得idf
            + df愈高，資訊量愈低，所以最後會算出idf = log(N/df)（底數10）
            + N 是總文章數，最極端是出現在每一篇文章，取log為0，無鑑別力；另一極端是出現1篇，其idf會很高
        2. collection frequency（總詞頻，出現在所有文章內次數，tf是單篇的次數）vs. document frequency（該辭彙出現篇數）
           + 若詞頻相等，df較低，其鑑別力就較高
           + 若密集被提及，代表其有意義，文字炭干最重要公式：
           `W = (1 + log tf) * log ( N / df )`
    * summary：布林值可排序，但要解決排序與權重問題，而採用tf-idf
## 向量空間模型 vector space model
* binary incidence matrix >> count matrix（紀錄總次數但為考量權重與意義）>> weight matrix（加權，相乘得到詞向量）
* 字詞數量決定空間維度，所以要先決定字詞要用哪些，所以會因為有多少字詞而變成向量
* 也可以先挑出來10個字，就能看到空間分佈，把不能解決的問題轉變過去
* 不過內部很多分量為0（稀疏），字詞數量維度很高
* 用處
    + 任兩點可計算夾角（相似度）跟距離
    + 計算文件密度
    + 找文件中心（代表性）
    + 進行分群（聚類）
    + 進行分類（歸類）（with trained model）
* 意義
    * 兩個字詞可以看到二維空間，可以先用內積算夾角 = `x1 * x2 + y1 * y2`，也可以用幾何距離（代表多接近） `((x1-x2)^2 + (y1-y2)^2)^1/2`
    * 分群（聚類）clustering：原本沒分群，自己因為相似度聚集
    * 分類（歸類）classification：將為分群新文章放入原有分群內
    * 中心 centroid：可以挑出某群最有代表性文章（特徵最核心）
    * 文件密度（了解文件分布狀況）：在單位空間內，知道出現某些字詞分布狀況）
* 修正討論
    * 詞之間可能有相依性，非垂直正交（orthogonal)，如內積結果是D1 = (1,0) D2 = (0,1)，內積為0，但可能相依，所以試圖要找正交軸
    * 高維度矩陣計算困難，除非使用GPU，所以有些詞不一定需要拿去分析，可以找有代表性的詞，設tf idf標準，也可以用降維方式，但結果有數學意義但閱讀困難（主成分分析等）
    * 若希望找到最相關文章，也都用向量表達，其假設：proximity = similarity = negative distance
    * 向量計算夾角會比幾何距離好：若有一篇文章複製貼上自己，資訊量沒變，夾角不便但幾何距離變遠，才不會因為膨脹造成偏誤；且**內幾比幾何距離快速，節省資源**
    #### 可以用cos計算夾角
    * 計算向量前要先正規化（除以自己的單位長度＝同一類別，不同向量平方相加，再開根號。使所有數值在0 1之間）
    * 算夾角 cos(q,d) = 內積/各自長度平方開根號之相乘
    * 若分量為0可以不用算，而且稀疏矩陣很多這樣狀況
* 最相關排序最前面

## 關聯分析
### 共現分析：要去思考原因

## 常見詞彙處理
* 取特徵詞feature selection: 選取有效鑑別的詞，以利後續分析處理（通常是用來機器學習、學習演算法等程式操作）
* 取關鍵詞keyword extraction: 選取最有代表性的詞（多半是為了給人類閱讀理解），人看到錯字會很敏感
* 切詞tokenization: 將內容切成多個單元，像是文章切多個句子，鋸子切成詞
* 斷詞word segmentation: 中文常見，因為沒有明確分隔符號，（多半依語意級文法）將內容做（正確且唯一的）切詞（只有一種切法）

* 哈利波特是好的關鍵字，但沒有鑑別力（不是好feature）

### 中文關鍵詞擷取
* 對自動摘要、文字雲都重要：可先斷詞或不先斷詞（適用外國人）
#### 不需先斷詞：N-gram approach

* 檢驗是否需要合併（要後處理）
##### 個人作業
1. 內容有特定語料區分，已去除雜數特殊符號，依照tf排序且僅保留tf >= 50者，有 2gram 與 3gram
2. lift：詞的密度
3. 優化tf-idf
   * 依領域修正 Mutual Information(MI)
        1. log(詞在特定類別總次數/(詞出現總次數*類別總次數))
        2. 因為總篇數固定所以沒差
        3. MI值愈高代表貢獻機率愈高，再計算tf-idf*MI
        4. 該詞在該類別事件`f(x,y) = df`, 該類別事件`f(x) = N`, 該詞之事件`f(y) = df in all corpus `, `MI = f(x,y)/(f(x)*f(y))`
        5. 可以計算更相關類別
   * 依分布修正 chi square（卡方）
        1. 若對照網路上資料（如，大語料庫），可以判斷哪些詞更突出，已解決IDF問題
        2. 卡方公式：加總所有（觀察值-期望值）^2，再除以（期望值） ，消除因為單篇被判定idf高的值，當卡方愈高代表這詞對該篇或該類特別有意義
        3. 換言之，卡方可以去除與「類別」不相關者（每個類別都會出現的），與IDF概念相同，但因為平方放大，效果更明顯
        4. 但因為有平方項，特別顯著跟不顯著都會被放大，所以要設法保留其正負
   * 依分布修正 lift
        1. 對照更大語料統計資料（背景知識），判斷哪些突出
        2. 公式 `lift = (該詞出現在該類別之篇數/該類別篇數）/(該詞出現之篇數/總篇數)`
